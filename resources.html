<html><head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
        <script type="text/javascript" src="https://netdna.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
        <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href="style.css" rel="stylesheet" type="text/css">
        <title>Publications</title>
    </head><body>
        <div class="section">
            <div class="container">
                <div class="row">
                    <div class="col-md-12">
                        <ul class="nav nav-pills">
                            <li>
                                <a href="index.html">Home</a>
                            </li>
                            <li>
                                <a href="resume.html">Resume</a>
                            </li>
                            <li class="active">
                                <a href="#">Publications<br></a>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <div class="section">
    <div class="container">
      <div class="row">
        <div class="col-md-4">
          <img src="sotp.png" class="img-responsive img-rounded"> </div>
        <div class="col-md-6">
            <p><b>Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation</b></p>
            <p>Advances in learning and representations have reinvigorated work that connects language to other modalities. A particularly exciting direction is Vision-and-Language Navigation (VLN), in which agents interpret natural language instructions and visual scenes to move through environments and reach goals. Despite recent progress, current research leaves unclear how much of a role language understanding plays in this task, especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions. Here, we highlight shortcomings of current metrics for the Room-to-Room dataset and propose a new metric, Coverage weighted by Length Score. We also show that the existing paths in the dataset are not ideal for evaluating instruction following because they are direct-to-goal shortest paths. We join existing short paths to create more challenging extended paths, and show that agents that receive rewards for instruction fidelity outperform agents that focus on goal completion.</p>
            <p><a href="https://scholar.google.co.in/citations?user=_IK8IzgAAAAJ&hl=en">Vihan Jain*</a>, <a href="http://www.gabrielilharco.com/">Gabriel Magalhaes*</a>, <a href="https://scholar.google.com/citations?user=Lh_ZqdcAAAAJ&hl=en">Alexander Ku*</a>, <a href="https://scholar.google.com/citations?user=6rUjwXUAAAAJ&hl=en">Ashish Vaswani</a>, <a href="https://scholar.google.com/citations?hl=en&user=jNCbl2IAAAAJ">Eugene Ie</a>, <a href="https://scholar.google.com/citations?hl=en&user=TP_JZm8AAAAJ">Jason Baldridge</a></p>
            <p>Published at <a href="http://www.acl2019.org/EN/index.xhtml">ACL 2019</a>.</p>
            <p><a href="https://arxiv.org/pdf/1905.12255.pdf">arxiv</a></p>
            <br>
        </div>
      </div>
      <div class="row">
        <div class="col-md-4">
          <img src="faces.png" class="img-responsive img-rounded"> </div>
        <div class="col-md-6">
            <p><b>Image Transformer</b></p>
            <p>Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.</p>
            <p><a href="https://scholar.google.com/citations?user=fpJHNQ8AAAAJ&hl=en">Niki Parmar</a>, <a href="https://scholar.google.com/citations?user=6rUjwXUAAAAJ&hl=en">Ashish Vaswani</a>, <a href="https://scholar.google.com/citations?user=duusroUAAAAJ&hl=en">Jakob Uszkoreit</a>, <a href="https://scholar.google.com/citations?user=JWmiQR0AAAAJ&hl=en">≈Åukasz Kaiser</a>, <a href="">Noam Shazeer</a>, <a href="https://scholar.google.com/citations?user=Lh_ZqdcAAAAJ&hl=en">Alexander Ku</a>, <a href="https://scholar.google.com/citations?user=wVazIm8AAAAJ&hl=en">Dustin Tran</a></p>
            <p>Published at <a href="https://icml.cc/Conferences/2018/Schedule?showEvent=2944">ICML 2018</a> and featured at <a href="https://youtu.be/zEOtG-ChmZE?list=PLOU2XLYxmsIInFRc3M44HUTQc3b_YJ4-Y&t=1229">Google I/O 2018</a>.</p>
            <p><a href="https://arxiv.org/abs/1802.05751">arxiv</a> | <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/image_transformer.py">code</a></p>
            <br>
        </div>
      </div>
      <div class="row">
        <div class="col-md-4">
          <img src="deepmcmcp.jpg" class="img-responsive img-rounded"> </div>
        <div class="col-md-6">
            <p><b>Capturing human category representations by sampling in deep feature spaces</b></p>
            <p>Understanding how people represent categories is a core problem in cognitive science. Decades of research have yielded a variety of formal theories of categories, but validating them with naturalistic stimuli is difficult. The challenge is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires a workable representation of these stimuli. Deep neural networks have recently been successful in solving a range of computer vision tasks and provide a way to compactly represent image features. Here, we introduce a method to estimate the structure of human categories that combines ideas from cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep image generators. We provide qualitative and quantitative results as a proof-of-concept for the method's feasibility. Samples drawn from human distributions rival those from state-of-the-art generative models in quality and outperform alternative methods for estimating the structure of human categories.</p>
            <p><a href="https://scholar.google.com/citations?user=ncbyhdMAAAAJ&hl=en">Joshua C. Peterson</a>, <a href="https://scholar.google.com/citations?user=S9xCl8EAAAAJ&hl=en">Jordan W. Suchow</a>, <a href="https://scholar.google.com/citations?user=KLcqPPgAAAAJ&hl=en">Krisha Aghi</a>, <a href="https://scholar.google.com/citations?user=Lh_ZqdcAAAAJ&hl=en">Alexander Y. Ku</a>, <a href="https://scholar.google.com/citations?user=UAwKvEsAAAAJ&hl=en">Thomas L. Griffiths</a></p>
            <p>Published at <a href="https://openreview.net/forum?id=BJy0fcgRZ">ICLR 2017</a> (workshop) and <a href="http://www.cognitivesciencesociety.org/conference/cogsci-2018/schedule/thursday-july-26th-schedule/">CogSci 2018</a> (conference).</p>
            <p><a href="https://arxiv.org/abs/1805.07644">arxiv</a></p>
            <br>
        </div>
      </div>
      <div class="row">
        <div class="col-md-4">
          <img src="deepvariant.png" class="img-responsive img-rounded"> </div>
        <div class="col-md-6">
            <p><b>Creating a universal SNP and small indel variant caller with deep neural networks</b></p>
            <p>Next-generation sequencing (NGS) is a rapidly evolving set of technologies that can be used to determine the sequence of an individual's genome by calling genetic variants present in an individual using billions of short, errorful sequence reads. Despite more than a decade of effort and thousands of dedicated researchers, the hand-crafted and parameterized statistical models used for variant calling still produce thousands of errors and missed variants in each genome. Here we show that a deep convolutional neural network can call genetic variation in aligned next-generation sequencing read data by learning statistical relationships (likelihoods) between images of read pileups around putative variant sites and ground-truth genotype calls. This approach, called DeepVariant, outperforms existing tools, even winning the "highest performance" award for SNPs in a FDA-administered variant calling challenge. The learned model generalizes across genome builds and even to other species, allowing non-human sequencing projects to benefit from the wealth of human ground truth data. We further show that, unlike existing tools which perform well on only a specific technology, DeepVariant can learn to call variants in a variety of sequencing technologies and experimental designs, from deep whole genomes from 10X Genomics to Ion Ampliseq exomes. DeepVariant represents a significant step from expert-driven statistical modeling towards more automatic deep learning approaches for developing software to interpret biological instrumentation data.</p>
            <p><a href="https://scholar.google.com/citations?user=yUzMq60AAAAJ&hl=en">Ryan Poplin</a>, <a href="https://scholar.google.com/citations?user=8_8omVoAAAAJ&hl=en">Pi-Chuan Chang</a>, <a href="https://scholar.google.com/citations?user=cG37rvIAAAAJ&hl=en">David Alexander</a>, <a href="https://scholar.google.com/citations?user=bm0mddQAAAAJ&hl=en">Scott Schwartz</a>, <a href="https://scholar.google.com/citations?user=1CpEjysAAAAJ&hl=en">Thomas Colthurst</a>, <a href="https://scholar.google.com/citations?user=Lh_ZqdcAAAAJ&hl=en">Alexander Ku</a>, <a href="https://scholar.google.com/citations?user=2jZMD1EAAAAJ&hl=en">Dan Newburger</a>, Jojo Dijamco, Nam Nguyen, <a href="https://scholar.google.com/citations?user=7dnBZQQAAAAJ&hl=en">Pegah T. Afshar</a>, <a href="">Sam S. Gross</a>, <a href="https://scholar.google.com/citations?user=74ZgDsUAAAAJ&hl=en">Lizzie Dorfman</a>, <a href="https://scholar.google.com/citations?user=PHgprNIAAAAJ&hl=en">Cory Y. McLean</a>, <a href="https://scholar.google.com/citations?user=OXefjc0AAAAJ&hl=en">Mark A. DePristo</a></p>
            <p>Published in <a href="https://www.nature.com/nbt/">Nature Biotechnology</a>.</p>
            <p><a href="https://www.biorxiv.org/content/early/2018/03/20/092890">biorxiv</a> | <a href="https://ai.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html">blog</a> | <a href="https://github.com/google/deepvariant">code</a></p>
            <br>
        </div>
      </div>
    </div>
</div>
    

</body></html>
