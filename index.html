<!DOCTYPE html>
<meta charset="utf-8">
<head>
  <link rel="icon" href="mepics/logo.png" tleype="image/gif" >
  <link rel="stylesheet" href="style.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.1/jquery.min.js"></script>
  <title>Alexander Ku</title>
</head>
<body>
  <div class="table">
    <div class="row">
      <div class="col-8">
        <h1 style="margin-top: 0;">Alexander Ku</h1>
        I'm a PhD student at Princeton, working with <a href="https://cocosci.princeton.edu/tom/index.php">Tom Griffiths</a> and <a href="https://jdc.princeton.edu/">Jon Cohen</a>,
        and a research scientist at Google DeepMind.
        <!-- <br><br> -->
        My research focuses on the principles underlying the flexibility and efficiency of human cognition, and whether these principles extend to large language models.
        Specifically, I study how the mind combines familiar concepts to solve unfamiliar problems (compositionality),
        how those concepts are represented and processed,
        and how these representations adapt to make solving familiar problems easier over time (automaticity).
        <!-- <br><br> -->
        <!-- Research interests: resource-rationality, reasoning, continual learning, skill acquisition, cognitive control, automaticity -->
        <br><br>
        <a href="mailto:alexku@princeton.edu">Email</a> /
        <a href="https://scholar.google.com/citations?user=Lh_ZqdcAAAAJ&hl=en">Google Scholar</a> /
        <a href="https://github.com/alexyku">GitHub</a> /
        <a href="https://twitter.com/alex_y_ku">Twitter</a> /
        <a href="alexku_cv.pdf">CV</a>
      </div>
      <div class="col-4">
        <img src="mepics/AlexKu_041025_0010.jpg">
      </div>
    </div>
    <div class="row">
      <div class="col-12">
        <h2>Selected papers</h2>
        Recent or representative papers:
        <br><br>
        <a href="https://arxiv.org/abs/2503.13401">Levels of analysis for large language models</a>
        <br>
        Alexander Ku, Declan Campbell, Xuechunzi Bai, Jiayi Geng, Ryan Liu, Raja Marjieh, R. Thomas McCoy, Andrew Nam, Ilia Sucholutsky, Veniamin Veselovsky, Liyi Zhang, Jian-Qiao Zhu, Thomas L. Griffiths
        <br><br>
        <a href="https://arxiv.org/abs/2505.09855">Predictability shapes adaptation: An evolutionary perspective on modes of learning in transformers</a>
        <br>
        Alexander Y. Ku, Thomas L. Griffiths, Stephanie C.Y. Chan
        <br><br>
        <a href="https://arxiv.org/abs/2505.00661">On the generalization of language models from in-context learning and finetuning: A controlled study</a>
        <br>
        Andrew K. Lampinen, Arslan Chaudhry, Stephanie C.Y. Chan, Cody Wild, Diane Wan, Alex Ku, JÃ¶rg Bornschein, Razvan Pascanu, Murray Shanahan, James L. McClelland
        <br><br>
      </div>
    </div>
  </div>
</body>
